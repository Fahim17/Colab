{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Real or Not? NLP with Disaster Tweets.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM8ueANkJoVdIGtnLe/LJZ3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iVCGwyrwxBqT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"status":"ok","timestamp":1591183494771,"user_tz":-360,"elapsed":14483,"user":{"displayName":"Fahimul Aleem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimXDh-YbgexzxdJhHN6HDnufrZk5CKsrGCZxab=s64","userId":"10981628195387930572"}},"outputId":"af80de1b-8a7b-4569-b736-53720bd47a39"},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle competitions download -c nlp-getting-started"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n","Downloading test.csv to /content\n","  0% 0.00/411k [00:00<?, ?B/s]\n","100% 411k/411k [00:00<00:00, 60.7MB/s]\n","Downloading sample_submission.csv to /content\n","  0% 0.00/22.2k [00:00<?, ?B/s]\n","100% 22.2k/22.2k [00:00<00:00, 22.7MB/s]\n","Downloading train.csv to /content\n","  0% 0.00/965k [00:00<?, ?B/s]\n","100% 965k/965k [00:00<00:00, 58.8MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QdwN_xe8zj1C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1591183500272,"user_tz":-360,"elapsed":3057,"user":{"displayName":"Fahimul Aleem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimXDh-YbgexzxdJhHN6HDnufrZk5CKsrGCZxab=s64","userId":"10981628195387930572"}},"outputId":"93682996-7082-46cb-b68c-75347e31813c"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OWX6QXOS3rDI","colab_type":"code","colab":{}},"source":["embedding_dim = 100\n","max_length = 16\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size= 7613\n","test_portion=.1\n","\n","corpus = []\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHyeegoq86U-","colab_type":"code","colab":{}},"source":["df_train = pd.read_csv('train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gy6kk75Wrl7Z","colab_type":"text"},"source":["# **Making Training dataset Approach 1**\n","Taking only test column to make dataset \n","\n","perform better\n"]},{"cell_type":"code","metadata":{"id":"gia6Q5sezmHH","colab_type":"code","colab":{}},"source":["sentence_tr = df_train.text\n","label_tr = df_train.target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7prMdfMIxCyT","colab_type":"text"},"source":["# **Making Training dataset Approach 2**\n","\n","Taking only text, location and keyword column to make dataset"]},{"cell_type":"code","metadata":{"id":"qjrXv-XhrPUM","colab_type":"code","colab":{}},"source":["df_train[['keyword']] = df_train[['keyword']].fillna('NoKey')\n","df_train[['location']] = df_train[['location']].fillna('NoLoc')\n","\n","sen = df_train.text\n","loc = df_train.location\n","keywd = df_train.keyword\n","label_tr = df_train.target\n","\n","sentence_tr = []\n","for i in range(len(sen)):\n","  tem = '{} {} {}'.format(keywd[i],loc[i],sen[i])\n","  sentence_tr.append(tem)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsfBPsxkzrXS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1591194760513,"user_tz":-360,"elapsed":1294,"user":{"displayName":"Fahimul Aleem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimXDh-YbgexzxdJhHN6HDnufrZk5CKsrGCZxab=s64","userId":"10981628195387930572"}},"outputId":"a9619115-b198-4178-9843-9a10377b1caa"},"source":["tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{\"}~\\t\\n', \n","                      oov_token = oov_tok,\n","                      lower = True,\n","                      split=\" \"\n","                      )\n","# tokenizer = Tokenizer(oov_token = oov_tok)\n","tokenizer.fit_on_texts(sentence_tr)\n","\n","word_index = tokenizer.word_index\n","vocab_size=len(word_index)\n","print(\"total number of words: {}\".format(vocab_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total number of words: 14037\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lM02th1yg8tC","colab_type":"code","colab":{}},"source":[" def standardize_text(df, text_field):\n","    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n","    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n","    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n","    df[text_field] = df[text_field].str.replace(r\"\\w*\\d+\\w*\", \"\")\n","    df[text_field] = df[text_field].str.replace(\"'\", \"\")\n","    df[text_field] = df[text_field].str.lower()\n","    return df\n","\n","df_train = standardize_text(df_train, 'text')\n","df_train = standardize_text(df_train, 'location')\n","df_train = standardize_text(df_train, 'keyword')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Cv33B2wxOCF","colab_type":"code","colab":{}},"source":["sorted(tokenizer.word_counts)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWh1SXbj1eTC","colab_type":"code","colab":{}},"source":["sequences = tokenizer.texts_to_sequences(sentence_tr)\n","padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","split = int(test_portion * training_size)\n","\n","test_sequences = padded[0:split]\n","training_sequences = padded[split:training_size]\n","test_labels = label_tr[0:split]\n","training_labels = label_tr[split:training_size]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdw4fpEO2I6h","colab_type":"code","colab":{}},"source":["# Note this is the 100 dimension version of GloVe from Stanford\n","# I unzipped and hosted it on my site to make this notebook easier\n","# *********************************\n","# \"word_index\" is a dic that map String WORD to Integer NUMBER\n","# \"embeddings_matrix\" is a dic that map Integer NUMBER to VECTOR\n","# *********************************\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n","    -O /tmp/glove.6B.100d.txt\n","embeddings_index = {};\n","with open('/tmp/glove.6B.100d.txt') as f:\n","    for line in f:\n","        values = line.split();\n","        word = values[0];\n","        coefs = np.asarray(values[1:], dtype='float32');\n","        embeddings_index[word] = coefs;\n","\n","embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word);\n","    if embedding_vector is not None:\n","        embeddings_matrix[i] = embedding_vector;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWog5qe52cwQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1591194795351,"user_tz":-360,"elapsed":14928,"user":{"displayName":"Fahimul Aleem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimXDh-YbgexzxdJhHN6HDnufrZk5CKsrGCZxab=s64","userId":"10981628195387930572"}},"outputId":"880dff89-0ce8-4b85-85d2-da23020cd293"},"source":["print(len(embeddings_matrix))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["14038\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gaK6tq8QzbAy","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import GRU\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import Conv1D\n","from tensorflow.keras.layers import MaxPooling1D\n","\n","from tensorflow.keras.constraints import unit_norm\n","from tensorflow.keras.constraints import max_norm\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7HUZmza3Vrv","colab_type":"code","colab":{}},"source":["model = Sequential()\n","\n","model.add(Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False))\n","model.add(Dropout(0.2))\n","# model.add(Conv1D(64, 5, activation= 'relu'))\n","# model.add(MaxPooling1D(pool_size=4))\n","# model.add(Bidirectional(GRU(32, return_sequences= True)))\n","model.add(Bidirectional(LSTM(32, return_sequences= True,)))\n","model.add(Bidirectional(LSTM(32)))\n","# model.add(Dense(32, activation = 'relu'))\n","# model.add(Dropout(0.2))\n","model.add(Dense(1, activation = 'sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66q6dNl3vd-3","colab_type":"code","colab":{}},"source":["num_epochs = 15\n","history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmybjH4-tmhZ","colab_type":"code","colab":{}},"source":["import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r')\n","plt.plot(epochs, val_acc, 'b')\n","plt.title('Training and validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r')\n","plt.plot(epochs, val_loss, 'b')\n","plt.title('Training and validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","plt.figure()\n","\n","\n","# Expected Output\n","# A chart where the validation loss does not increase sharply!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OaG-G9Lao3rD","colab_type":"code","colab":{}},"source":["df_train[25:50]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vr0Qm88rp4kI","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}